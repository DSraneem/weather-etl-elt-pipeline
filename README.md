# ğŸŒ¦ï¸ Weather ETL & ELT Data Pipeline

## ğŸ“Œ Project Overview
This project demonstrates the design and implementation of **both ETL (Extractâ€“Transformâ€“Load) and ELT (Extractâ€“Loadâ€“Transform) pipelines** for integrating and processing weather data from heterogeneous sources.

The pipeline combines **real-time weather data from an API** with **historical data from CSV files**, applies data validation and quality checks, and prepares analytics-ready datasets for business intelligence and analysis.

The project emphasizes **data quality, scalability, and modern data engineering practices**.

---

## ğŸ¯ Objectives
- Build a complete **ETL pipeline** for structured data processing
- Implement an **ELT pipeline** for scalable and flexible transformations
- Integrate data from **API and file-based sources**
- Apply **data validation using Great Expectations**
- Compare ETL vs. ELT in terms of **speed, scalability, and efficiency**

---

## ğŸ“‚ Data Sources
- **Visual Crossing Weather API**  
  - JSON-formatted real-time and historical weather data
- **CSV Files**  
  - Structured historical weather datasets

Both sources include timestamps and location metadata to support data lineage and integration.

---

## âš™ï¸ ETL Pipeline Architecture
**Extract â†’ Transform â†’ Load**

### ğŸ”¹ Extract
- API ingestion with retry logic and response caching
- CSV ingestion using Pandas with schema detection

### ğŸ”¹ Transform
- Column standardization and unit normalization
- Handling missing values and outliers
- Schema-aware merging of multi-source data
- Feature enrichment and metadata tagging

### ğŸ”¹ Load
- Cleaned data stored in **SQLite**
- Batch loading using Pandas and SQLAlchemy

---

## ğŸ”„ ELT Pipeline Architecture
**Extract â†’ Load â†’ Transform**

### ğŸ”¹ Extract & Load
- Raw data loaded directly into **PostgreSQL staging tables**
- Separate staging tables for each data source

### ğŸ”¹ Validation
- Data quality checks using **Great Expectations**
- Schema validation and range checks (e.g., humidity 0â€“100)

### ğŸ”¹ Transform
- SQL-based transformations executed inside the data warehouse
- Data type enforcement, null handling, and standardization
- Creation of analysis-ready tables

---

## ğŸ“Š Data Validation & Quality
The project applies multi-layered data quality checks:
- Schema validation
- Statistical range checks

---

## ğŸ› ï¸ Tools & Technologies
- **Python**
- **Pandas, NumPy**
- **SQLAlchemy**
- **Great Expectations**
- **SQLite**
- **PostgreSQL**
- **REST APIs**
- **SQL (in-database transformations)**

---

## ğŸ“ˆ Visualization
Processed datasets were visualized using **BI dashboards** to support analytics and decision-making.

---

## ğŸ” ETL vs. ELT Comparison

| Aspect        | ETL                          | ELT                          |
|--------------|------------------------------|------------------------------|
| Processing   | Transform before load        | Transform after load         |
| Scalability  | Limited by processing layer  | Scales with data warehouse   |
| Flexibility  | Rigid schema                 | Schema-on-read               |
| Use Case     | Structured datasets          | Large & diverse datasets     |

---

## ğŸŒ Sustainability Impact
- **SDG 13 â€“ Climate Action**: Supports accurate climate monitoring and weather analysis

---

## âœ… Key Skills Demonstrated
- ETL & ELT pipeline design
- API and CSV data integration
- Data validation and quality engineering
- SQL-based transformations
- Modern data engineering architecture

---
